{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "13387c7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow TabNet is ready!\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(r\"D:\\Recyclogy\\Own models\\tf-TabNet\")\n",
    "\n",
    "from tabnet.tabnet import TabNetClassifier\n",
    "\n",
    "print(\"TensorFlow TabNet is ready!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2dbd553",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available: 1\n",
      "GPUs: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "[TabNet]: 32 features will be used for decision steps.\n",
      "Epoch 1/100\n",
      "79/79 [==============================] - 20s 95ms/step - loss: 3.8259 - accuracy: 0.0221 - val_loss: 3.8080 - val_accuracy: 0.0230\n",
      "Epoch 2/100\n",
      "79/79 [==============================] - 5s 66ms/step - loss: 3.8072 - accuracy: 0.0227 - val_loss: 3.8069 - val_accuracy: 0.0222\n",
      "Epoch 3/100\n",
      "79/79 [==============================] - 5s 66ms/step - loss: 3.8068 - accuracy: 0.0222 - val_loss: 3.8068 - val_accuracy: 0.0215\n",
      "Epoch 4/100\n",
      "79/79 [==============================] - 5s 65ms/step - loss: 3.8063 - accuracy: 0.0233 - val_loss: 3.8069 - val_accuracy: 0.0228\n",
      "Epoch 5/100\n",
      "79/79 [==============================] - 5s 68ms/step - loss: 3.8061 - accuracy: 0.0228 - val_loss: 3.8069 - val_accuracy: 0.0223\n",
      "Epoch 6/100\n",
      "79/79 [==============================] - 6s 74ms/step - loss: 3.8059 - accuracy: 0.0229 - val_loss: 3.8076 - val_accuracy: 0.0210\n",
      "Epoch 7/100\n",
      "79/79 [==============================] - 7s 85ms/step - loss: 3.8058 - accuracy: 0.0236 - val_loss: 3.8077 - val_accuracy: 0.0216\n",
      "Epoch 8/100\n",
      "79/79 [==============================] - 7s 84ms/step - loss: 3.8059 - accuracy: 0.0238 - val_loss: 3.8073 - val_accuracy: 0.0206\n",
      "Epoch 9/100\n",
      "79/79 [==============================] - 6s 78ms/step - loss: 3.8063 - accuracy: 0.0231 - val_loss: 3.8070 - val_accuracy: 0.0212\n",
      "Epoch 10/100\n",
      "79/79 [==============================] - 6s 79ms/step - loss: 3.8066 - accuracy: 0.0220 - val_loss: 3.8068 - val_accuracy: 0.0214\n",
      "Epoch 11/100\n",
      "79/79 [==============================] - 6s 81ms/step - loss: 3.8066 - accuracy: 0.0226 - val_loss: 3.8067 - val_accuracy: 0.0221\n",
      "Epoch 12/100\n",
      "79/79 [==============================] - 7s 89ms/step - loss: 3.8066 - accuracy: 0.0223 - val_loss: 3.8067 - val_accuracy: 0.0221\n",
      "Epoch 13/100\n",
      "79/79 [==============================] - 7s 83ms/step - loss: 3.8065 - accuracy: 0.0221 - val_loss: 3.8066 - val_accuracy: 0.0221\n",
      "Epoch 14/100\n",
      "79/79 [==============================] - 7s 88ms/step - loss: 3.8065 - accuracy: 0.0222 - val_loss: 3.8067 - val_accuracy: 0.0220\n",
      "Epoch 15/100\n",
      "79/79 [==============================] - 6s 75ms/step - loss: 3.8065 - accuracy: 0.0224 - val_loss: 3.8066 - val_accuracy: 0.0220\n",
      "Epoch 16/100\n",
      "79/79 [==============================] - 6s 77ms/step - loss: 3.8065 - accuracy: 0.0221 - val_loss: 3.8066 - val_accuracy: 0.0220\n",
      "Epoch 17/100\n",
      "79/79 [==============================] - 6s 78ms/step - loss: 3.8065 - accuracy: 0.0224 - val_loss: 3.8066 - val_accuracy: 0.0220\n",
      "Epoch 18/100\n",
      "79/79 [==============================] - 6s 80ms/step - loss: 3.8065 - accuracy: 0.0221 - val_loss: 3.8066 - val_accuracy: 0.0220\n",
      "Epoch 19/100\n",
      "79/79 [==============================] - 7s 85ms/step - loss: 3.8065 - accuracy: 0.0222 - val_loss: 3.8066 - val_accuracy: 0.0220\n",
      "Epoch 20/100\n",
      "79/79 [==============================] - 5s 68ms/step - loss: 3.8065 - accuracy: 0.0223 - val_loss: 3.8066 - val_accuracy: 0.0224\n",
      "Epoch 21/100\n",
      "79/79 [==============================] - 5s 69ms/step - loss: 3.8065 - accuracy: 0.0222 - val_loss: 3.8066 - val_accuracy: 0.0220\n",
      "Epoch 22/100\n",
      "79/79 [==============================] - 6s 72ms/step - loss: 3.8065 - accuracy: 0.0224 - val_loss: 3.8067 - val_accuracy: 0.0220\n",
      "Epoch 23/100\n",
      "79/79 [==============================] - 6s 73ms/step - loss: 3.8065 - accuracy: 0.0224 - val_loss: 3.8066 - val_accuracy: 0.0224\n",
      "Epoch 24/100\n",
      "79/79 [==============================] - 6s 74ms/step - loss: 3.8065 - accuracy: 0.0221 - val_loss: 3.8066 - val_accuracy: 0.0224\n",
      "Epoch 25/100\n",
      "79/79 [==============================] - 7s 84ms/step - loss: 3.8065 - accuracy: 0.0223 - val_loss: 3.8066 - val_accuracy: 0.0224\n",
      "Epoch 26/100\n",
      "79/79 [==============================] - 6s 76ms/step - loss: 3.8065 - accuracy: 0.0224 - val_loss: 3.8066 - val_accuracy: 0.0224\n",
      "Epoch 27/100\n",
      "79/79 [==============================] - 6s 73ms/step - loss: 3.8065 - accuracy: 0.0224 - val_loss: 3.8066 - val_accuracy: 0.0224\n",
      "Epoch 28/100\n",
      "79/79 [==============================] - 6s 70ms/step - loss: 3.8065 - accuracy: 0.0223 - val_loss: 3.8066 - val_accuracy: 0.0224\n",
      "Epoch 29/100\n",
      "79/79 [==============================] - 6s 72ms/step - loss: 3.8065 - accuracy: 0.0223 - val_loss: 3.8066 - val_accuracy: 0.0220\n",
      "Epoch 30/100\n",
      "79/79 [==============================] - 5s 70ms/step - loss: 3.8065 - accuracy: 0.0222 - val_loss: 3.8066 - val_accuracy: 0.0220\n",
      "Epoch 31/100\n",
      "79/79 [==============================] - 6s 70ms/step - loss: 3.8065 - accuracy: 0.0224 - val_loss: 3.8066 - val_accuracy: 0.0220\n",
      "Epoch 32/100\n",
      "79/79 [==============================] - 6s 71ms/step - loss: 3.8065 - accuracy: 0.0223 - val_loss: 3.8066 - val_accuracy: 0.0224\n",
      "Epoch 33/100\n",
      "79/79 [==============================] - 5s 69ms/step - loss: 3.8065 - accuracy: 0.0224 - val_loss: 3.8066 - val_accuracy: 0.0220\n",
      "Epoch 34/100\n",
      "79/79 [==============================] - 5s 69ms/step - loss: 3.8065 - accuracy: 0.0221 - val_loss: 3.8066 - val_accuracy: 0.0224\n",
      "625/625 [==============================] - 15s 22ms/step\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Classification metrics can't handle a mix of multiclass and continuous-multioutput targets",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[29], line 88\u001b[0m\n\u001b[0;32m     86\u001b[0m \u001b[38;5;66;03m# ✅ Evaluation\u001b[39;00m\n\u001b[0;32m     87\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m tabnet_model\u001b[38;5;241m.\u001b[39mpredict(X_valid_dict)\n\u001b[1;32m---> 88\u001b[0m accuracy \u001b[38;5;241m=\u001b[39m \u001b[43maccuracy_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_valid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     89\u001b[0m f1 \u001b[38;5;241m=\u001b[39m f1_score(y_valid, y_pred, average\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mweighted\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     91\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mValidation Accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maccuracy\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\py_gpu\\lib\\site-packages\\sklearn\\utils\\_param_validation.py:218\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    212\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    213\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m    214\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    215\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    216\u001b[0m         )\n\u001b[0;32m    217\u001b[0m     ):\n\u001b[1;32m--> 218\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    219\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    220\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[0;32m    221\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[0;32m    222\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[0;32m    223\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[0;32m    224\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[0;32m    225\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    226\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    227\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[0;32m    228\u001b[0m     )\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\py_gpu\\lib\\site-packages\\sklearn\\metrics\\_classification.py:359\u001b[0m, in \u001b[0;36maccuracy_score\u001b[1;34m(y_true, y_pred, normalize, sample_weight)\u001b[0m\n\u001b[0;32m    357\u001b[0m \u001b[38;5;66;03m# Compute accuracy for each possible representation\u001b[39;00m\n\u001b[0;32m    358\u001b[0m y_true, y_pred \u001b[38;5;241m=\u001b[39m attach_unique(y_true, y_pred)\n\u001b[1;32m--> 359\u001b[0m y_type, y_true, y_pred \u001b[38;5;241m=\u001b[39m \u001b[43m_check_targets\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    360\u001b[0m check_consistent_length(y_true, y_pred, sample_weight)\n\u001b[0;32m    362\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y_type\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmultilabel\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\py_gpu\\lib\\site-packages\\sklearn\\metrics\\_classification.py:106\u001b[0m, in \u001b[0;36m_check_targets\u001b[1;34m(y_true, y_pred)\u001b[0m\n\u001b[0;32m    103\u001b[0m     y_type \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmulticlass\u001b[39m\u001b[38;5;124m\"\u001b[39m}\n\u001b[0;32m    105\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(y_type) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m--> 106\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    107\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mClassification metrics can\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt handle a mix of \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m and \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m targets\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m    108\u001b[0m             type_true, type_pred\n\u001b[0;32m    109\u001b[0m         )\n\u001b[0;32m    110\u001b[0m     )\n\u001b[0;32m    112\u001b[0m \u001b[38;5;66;03m# We can't have more than one value on y_type => The set is no more needed\u001b[39;00m\n\u001b[0;32m    113\u001b[0m y_type \u001b[38;5;241m=\u001b[39m y_type\u001b[38;5;241m.\u001b[39mpop()\n",
      "\u001b[1;31mValueError\u001b[0m: Classification metrics can't handle a mix of multiclass and continuous-multioutput targets"
     ]
    }
   ],
   "source": [
    "# ✅ Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder, MultiLabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import tensorflow as tf\n",
    "import sys\n",
    "\n",
    "# Add tf-TabNet path\n",
    "sys.path.append(r\"D:\\Recyclogy\\Own models\\tf-TabNet\")\n",
    "from tabnet.tabnet import TabNetClassifier\n",
    "\n",
    "# ⚡ Check GPU\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "print(\"Num GPUs Available:\", len(gpus))\n",
    "print(\"GPUs:\", gpus)\n",
    "\n",
    "# 📊 Load Dataset\n",
    "df = pd.read_csv('waste_product_v1.csv')\n",
    "\n",
    "# 🧱 Preprocessing\n",
    "target_encoder = LabelEncoder()\n",
    "df['Project_Label'] = target_encoder.fit_transform(df['Project'])\n",
    "\n",
    "df['Waste_Items_List'] = df['Waste Items'].apply(lambda x: [item.strip() for item in x.split(',')])\n",
    "df['Tools_List'] = df['Tools'].apply(lambda x: [item.strip() for item in x.split(',')])\n",
    "\n",
    "mlb_waste = MultiLabelBinarizer()\n",
    "mlb_tools = MultiLabelBinarizer()\n",
    "\n",
    "waste_features = mlb_waste.fit_transform(df['Waste_Items_List'])\n",
    "tools_features = mlb_tools.fit_transform(df['Tools_List'])\n",
    "\n",
    "X = np.hstack([waste_features, tools_features])\n",
    "y = df['Project_Label'].values\n",
    "\n",
    "# Split dataset\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Convert features to dict format\n",
    "def array_to_dict(X_array):\n",
    "    return {str(i): X_array[:, i] for i in range(X_array.shape[1])}\n",
    "\n",
    "X_train_dict = array_to_dict(X_train)\n",
    "X_valid_dict = array_to_dict(X_valid)\n",
    "\n",
    "# TabNet feature columns\n",
    "input_dim = X_train.shape[1]\n",
    "feature_columns = [tf.feature_column.numeric_column(str(i)) for i in range(input_dim)]\n",
    "\n",
    "# Initialize TabNet\n",
    "num_classes = len(np.unique(y_train))\n",
    "tabnet_model = TabNetClassifier(\n",
    "    feature_columns=feature_columns,\n",
    "    num_classes=num_classes,\n",
    "    feature_dim=64,\n",
    "    output_dim=32,\n",
    "    num_decision_steps=5,\n",
    "    relaxation_factor=1.5\n",
    ")\n",
    "\n",
    "# ✅ Compile the model (required!)\n",
    "tabnet_model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=2e-2),\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# ⏱ Train\n",
    "early_stop = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss', patience=10, restore_best_weights=True\n",
    ")\n",
    "\n",
    "tabnet_model.fit(\n",
    "    X_train_dict, y_train,\n",
    "    validation_data=(X_valid_dict, y_valid),\n",
    "    batch_size=1024,\n",
    "    epochs=100,\n",
    "    callbacks=[early_stop],\n",
    "    verbose=1\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9d29bdd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "625/625 [==============================] - 12s 18ms/step\n",
      "Validation Accuracy: 0.0224\n",
      "Validation F1-Score: 0.0019\n",
      "1/1 [==============================] - 0s 62ms/step\n",
      "Suggested DIY Project: Creative Bracelet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Anaconda\\envs\\py_gpu\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:909: UserWarning: unknown class(es) ['Glue'] will be ignored\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# ✅ Evaluation\n",
    "y_pred_probs = tabnet_model.predict(X_valid_dict)  # shape: (num_samples, num_classes)\n",
    "y_pred = np.argmax(y_pred_probs, axis=1)           # convert to class labels\n",
    "\n",
    "accuracy = accuracy_score(y_valid, y_pred)\n",
    "f1 = f1_score(y_valid, y_pred, average='weighted')\n",
    "\n",
    "print(f'Validation Accuracy: {accuracy:.4f}')\n",
    "print(f'Validation F1-Score: {f1:.4f}')\n",
    "\n",
    "# ✅ Inference\n",
    "def predict_project(waste_items, tools):\n",
    "    waste_vec = mlb_waste.transform([waste_items])\n",
    "    tools_vec = mlb_tools.transform([tools])\n",
    "    X_input = np.hstack([waste_vec, tools_vec])\n",
    "    X_input_dict = array_to_dict(X_input)\n",
    "    \n",
    "    pred_probs = tabnet_model.predict(X_input_dict)\n",
    "    pred_label = np.argmax(pred_probs, axis=1)[0]   # convert to class index\n",
    "    project_name = target_encoder.inverse_transform([pred_label])[0]\n",
    "    \n",
    "    return project_name\n",
    "\n",
    "\n",
    "# Example\n",
    "new_waste_items = [\"Ribbon Scrap\", \"Rope Piece\"]\n",
    "new_tools = [\"Glue\", \"Scissors\"]\n",
    "\n",
    "predicted_project = predict_project(new_waste_items, new_tools)\n",
    "print(f\"Suggested DIY Project: {predicted_project}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
